{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSWEcS2XKgzi"
      },
      "source": [
        "### Practice: Parameter Efficient Fine-Tuning\n",
        "In this notebook, you're gonna fine-tune large language models within limited GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xeRF_hSKgzs"
      },
      "outputs": [],
      "source": [
        "# %pip install --quiet transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "\n",
        "import transformers\n",
        "from tqdm.auto import tqdm, trange\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "536530f44f5448e99bfaea94f55f3ffb",
            "2b297f4b29ac4dbc9d5f007457920b15",
            "6d63e403db9a4d6dacbdec3ae08e088a",
            "a2b4f4b9da5945f383eaa7fbc23bf170",
            "27ec6cec8e7c467a94f044c5a132ee49",
            "c0ae52a6ce8e43a0a52e6267ec3b27bb",
            "fa6142454e7f4bebb662d4284affbb39",
            "130102f1843042059d72779bba16c51b",
            "aa46f51405a94efd8860c4d1023112be",
            "bd06a07f64204ae684f917e2cc04e213",
            "6bf3f2983ad444f79798ea2f6b9f4f03",
            "88a8daca68be40f5b7a622a7a96dc7c1"
          ]
        },
        "id": "VMzFwx29Kgzu",
        "outputId": "da99648b-2c1d-41f6-fee2-9e8943e4b167"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88a8daca68be40f5b7a622a7a96dc7c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ... and the model itself\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
        "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgspB2JwSIS2"
      },
      "source": [
        "### Prompt tuning: the story of a fox (2 pts)\n",
        "\n",
        "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H13pYFRxQi4U",
        "outputId": "a97a9e5c-f66e-415f-c237-bac67d14a930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>A quick brown fox jumps over the lazy dog.\n",
            "A quick\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVhZACT6SgLq"
      },
      "source": [
        "What a blatant lie! This particular fox assures you that it didn't in fact jump over the lazy dog. No, sir! The fox was just minding its own business. __Your task is to train the model to say truth: no dog was jumped over today.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r6UVDl4NEua",
        "outputId": "05ec4c1a-2450-43c7-fcef-d346a009c3e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: tensor(3.0725, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "outputs = model(**batch)\n",
        "\n",
        "next_word_logits = outputs.logits[:, :-1]\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "print(\"Loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvNufS8WXa0"
      },
      "source": [
        "Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n",
        "\n",
        "![img](https://i.imgur.com/VwNNKnb.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73ZOCFRZWR98"
      },
      "outputs": [],
      "source": [
        "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
        "    \"\"\"\n",
        "    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer\n",
        "     - that inserts trainable prompts instead of the first N token embeddings. \"\"\"\n",
        "\n",
        "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
        "        super().__init__()\n",
        "        self.original_word_embeddings = word_embeddings\n",
        "        self.num_prompts = num_prompts\n",
        "        self.learnable_prompts = nn.Parameter(\n",
        "            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor):\n",
        "        # input_ids shape: [batch_size, seq length]\n",
        "        assert input_ids.dtype == torch.int64\n",
        "        assert input_ids.shape[1] > self.num_prompts\n",
        "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \"don't forget to prepend several BOS tokens to input_ids\"\n",
        "\n",
        "        # Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts\n",
        "        # This is because we will prepend :num_prompts: padding tokens at the beginning\n",
        "\n",
        "        # After you are done, you must produce a word embedding vector for each token in input_ids,\n",
        "        # except that the first :num_prompts: vectors should equal learnable_prompts;\n",
        "        # any additional vectors after first :num_prompts: ones should be embedded as usual\n",
        "        # Note: since you're dealing with trainable params, please torch.cat instead of item assignment\n",
        "\n",
        "        #<YOUR CODE HERE>\n",
        "        first_embeds = self.learnable_prompts\n",
        "        # print(first_embeds)\n",
        "        # print(first_embeds.shape)\n",
        "        last_embeds = self.original_word_embeddings(input_ids[:, self.num_prompts:])\n",
        "        # print(last_embeds)\n",
        "        # print(last_embeds.shape)\n",
        "        embeds = torch.cat((first_embeds, last_embeds), dim=1)\n",
        "\n",
        "        return embeds #embeds #your_outputs_with_prompts_as_per_instructions_above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxUyUU2uT2f1",
        "outputId": "ea89636b-b938-4127-cfbc-0ddffaf1ed40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks legit!\n"
          ]
        }
      ],
      "source": [
        "num_prompts = 16\n",
        "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
        "test_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n",
        "\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
        "\n",
        "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
        "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
        "assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n",
        "assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n",
        "print(\"Looks legit!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbKPgfT-crqW"
      },
      "source": [
        "__Now that it works,__ let's inject learnable prompts into the main model and teach it about foxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRe0lpREV49G"
      },
      "outputs": [],
      "source": [
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n",
        "\n",
        "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
        "\n",
        "opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gVQzgdka-Bm",
        "outputId": "c7b63fca-1a42-4026-8748-47a956e791e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: tensor(7.5012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(7.5012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(6.6436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(6.0970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.2714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.9178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.5674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.2750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.0833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.9245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.7696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.5967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.4106, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.2528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.1115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.9750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.8430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.5874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1336, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "outputs = model(**batch)\n",
        "next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "\n",
        "#raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n",
        "while loss.item() >= 0.1:\n",
        "    opt.zero_grad()\n",
        "    outputs = model(**batch)\n",
        "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "    print(\"Loss:\", loss)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "\n",
        "assert loss.item() <= 0.1\n",
        "print(\"Good job!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7DkWHD-r1Xo",
        "outputId": "390b6ed7-0e09-47c0-eeb2-d7ff4e32f2a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "\n",
        "for i in range(15):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n",
        "\n",
        "# if you did everything right, the model will deny that the fox jumped over the lazy dog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEkoFNdlshv_"
      },
      "source": [
        "### Using HuggingFace PEFT (2 points)\n",
        "\n",
        "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXytkhv71_Ky"
      },
      "outputs": [],
      "source": [
        "device='cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "165d73419e5248d1940c7a60f815625f",
            "d675431174cd439c8a8b6a0610ce364d",
            "e16436c4c8b44f11bd1cf8f559cf95ff",
            "62c679549dde4557a61759922de28de5",
            "435dd8a2143f4d04bf4f72acad88d75c",
            "9a4db375ca6e40828a61a59cdbd796be",
            "79fc63080fc14e3c8cc435c7a6ca31cb",
            "9ac5c842761e414091f676d3dc190ad3",
            "fa00dc286ee64cef9f84d8debeab18f9",
            "df3af726188c43abbd2853cfc7fab66f",
            "a44ebdcc28244d8890e37a9ca15cf0db"
          ]
        },
        "id": "zoHJgsGsziYf",
        "outputId": "149b614c-aecb-451d-a239-895b92d56689"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "165d73419e5248d1940c7a60f815625f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ... and the model itself\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
        "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqEEpZm2Q4UC",
        "outputId": "b97bccab-0558-4d01-93f4-9ad11d36c9ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 65536\n",
            "Total parameters (excluding quantization): 3500478464\n"
          ]
        }
      ],
      "source": [
        "import peft\n",
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
        "\n",
        "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
        "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
        "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW54GnzCwVpp"
      },
      "outputs": [],
      "source": [
        "# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n",
        "# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n",
        "# Finally, generate the sentence to make sure that the model learned the truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71vJ9Mq7w67f"
      },
      "outputs": [],
      "source": [
        "# Feel free to structure your code as you see fit - as long as it's legible :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WuFA1yhqnsv",
        "outputId": "431843bf-6bf3-45aa-8514-4748ce72ae72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: tensor(7.5610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(7.5610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(6.9693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(6.3131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.8251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.4258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.8803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.4750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.2894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.1095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.9321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.7543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.5746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.3935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.2133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.0367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.8655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.5409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "num_virtual_tokens = 16\n",
        "\n",
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "\n",
        "outputs = model(**batch)\n",
        "next_word_logits = outputs.logits[:, num_virtual_tokens : -1, :]\n",
        "# print(next_word_logits.shape)\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "# print(true_next_tokens.shape)\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "while loss.item() >= 0.1:\n",
        "    outputs = model(**batch)\n",
        "    next_word_logits = outputs.logits[:, num_virtual_tokens : -1, :]\n",
        "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "    print(\"Loss:\", loss)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "assert loss.item() <= 0.1\n",
        "print(\"Good job!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5KvFVlvqnsw",
        "outputId": "20b942e6-8f56-4971-fe28-62bf1638d34b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(17):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCkpKYjWxfhk"
      },
      "source": [
        "### Parameter-efficient finetuning with LoRA (2 points)\n",
        "\n",
        "When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
        "\n",
        "The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n",
        "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n",
        "\n",
        "In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIUV98mCqnsz",
        "outputId": "0d93f2c9-e0ec-462b-972d-9b8f22b14d1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8e49394e45e940428e7c84978164815f",
            "0756ee6cb8dc4ea5a2805f0e0d7d2ce8",
            "5e7c20e77b9649f0ad69ca6ca4760f09",
            "86ab3b4df59c4c989795224a10cdb7e0",
            "aee1bb89f43742239ad0ac3b8c0a55d7",
            "7e259febd15243b78ce9c8d81c65946c",
            "d8f842f7942e41488a73f7d1ce2b5298",
            "085b6c7dcd794fa182aa2b6a18dd0c14",
            "012d2d2de5b648e184287a82c4a5b4a5",
            "706f7a7dbdc3482a9614be8aea322573",
            "1a7debb4a3d949c585a0606cbeec17f8"
          ]
        },
        "id": "8zundaSzx90r",
        "outputId": "225f0201-38c9-4bba-8ebd-bb7fb4458436"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e49394e45e940428e7c84978164815f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# re-load the model to remove any previous PEFT tuners\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ_hq4fwyPVR"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module  # pre-trained (frozen) linear layer\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
        "        #  <YOUR CODE HERE>\n",
        "        pretrained_output = self.module(input)\n",
        "        lora_output = torch.matmul(input, torch.matmul(self.adapter_A, self.adapter_B))\n",
        "        return pretrained_output + lora_output # <YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTzOs65JydcS",
        "outputId": "bba51e40-adb9-473b-9d19-130f3274b1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# test your implementation\n",
        "test_linear = nn.Linear(128, 128)\n",
        "test_linear.weight.data[...] = torch.eye(128)\n",
        "test_adapter = LoRALayer(test_linear, rank=8)\n",
        "\n",
        "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
        "\n",
        "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
        "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
        "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
        "\n",
        "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
        "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
        "dummy_loss.backward()\n",
        "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
        "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
        "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
        "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
        "del dummy_loss, test_linear, test_adapter\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tajVTsvLulB6"
      },
      "source": [
        "### Apply LoRA to the model\n",
        "\n",
        "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
        "* self_attn.o_proj - attention output projection\n",
        "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
        "* lm_head - output LM head\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "davyUVEwulB6"
      },
      "outputs": [],
      "source": [
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "\n",
        "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWzfvc0EulB6",
        "outputId": "2f505729-6b19-4f3f-d4fa-79a490d5f3f1",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grad check successful, well done!\n"
          ]
        }
      ],
      "source": [
        "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n",
        "# test a single training step, make sure we get meaningful gradients\n",
        "with torch.cuda.amp.autocast(dtype=torch.float32):\n",
        "\n",
        "    out = model.forward(**batch)\n",
        "    (out.logits.norm() / 100).backward()\n",
        "\n",
        "for i, module in enumerate(model.modules()):\n",
        "    if isinstance(module, LoRALayer):\n",
        "        assert module.adapter_B.grad is not None\n",
        "        assert module.adapter_B.grad.norm().item() > 0\n",
        "\n",
        "model.zero_grad(set_to_none=True)\n",
        "print(\"Grad check successful, well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjIJ1vkUulB7"
      },
      "source": [
        "### (example) How to train your model\n",
        "\n",
        "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9mIpntHulB8",
        "outputId": "6eeb75f2-f314-4d90-afae-8f89b24c3399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "“Be yourself; everyone else is already taken.”\n",
            "<list_iterator object at 0x7cb850d8d8a0>\n"
          ]
        }
      ],
      "source": [
        "# checking if the model can learn. Change max_steps for proper training\n",
        "import datasets\n",
        "data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") # 32 lines\n",
        "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
        "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, train_dataset=data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2, gradient_accumulation_steps=1,\n",
        "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
        "        warmup_steps=250, max_steps=100, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=1, output_dir='outputs', report_to=None),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQUlqoEAulB8"
      },
      "source": [
        "### Final task: *actually* train the model (4 points)\n",
        "\n",
        "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n",
        "\n",
        "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n",
        "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n",
        "* __short lines:__ please take the first 512 characters of each line\n",
        "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n",
        "   - extra adapter on lm_head\n",
        "   - extra adapter on MLP components (mlp.*)\n",
        "   - trainable input embeddings (requires tweaking memory usage)\n",
        "\n",
        "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n",
        "\n",
        "\n",
        "Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n",
        "\n",
        "__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DZNTM4F6cUf"
      },
      "source": [
        "Inspiration: https://huggingface.co/blog/codeparrot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LfFWSYhulB8"
      },
      "outputs": [],
      "source": [
        "prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
        "\n",
        "# <A WHOLE LOT OF YOUR CODE>\n",
        "# generate baseline samples with the selected prompts before finetuning\n",
        "# please feel free to use transformers.Trainer (as above) or your custom training code\n",
        "# after the training concludes, please show examples of text generated by your model. It is expected to look like Python code fragments\n",
        "# print the generation examples nicely (suggestion: use pandas or HTML) for easier comparison\n",
        "# note: your LoRA-enhanced model can run generation the same way as the non-trained model (above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAncn_k4p_Kb",
        "outputId": "a748abfc-22a4-4f76-895e-97a968d15e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: ▶▶ 2019-2020 School Year\n",
            "The 20\n",
            "\n",
            "Output: import Foundation\n",
            "\n",
            "public extension NSURL {\n",
            "    public var absoluteString: String {\n",
            "\n",
            "\n",
            "Output: from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from\n",
            "\n",
            "Output: while(1)\n",
            "while(1) {\n",
            "    // do something\n",
            "}\n",
            "\\\n",
            "\n",
            "Output: try to find the best solution for your needs.\n",
            "We are a team of professionals with\n",
            "\n",
            "Output: if ( !window.atmosphere ) {\n",
            "    window.atmosphere = {};\n",
            "\n",
            "Output: for the 2019-2020 school year.\n",
            "The application process\n",
            "\n",
            "Output: torchbearer 2017-05-18 19\n"
          ]
        }
      ],
      "source": [
        "# before finetuning\n",
        "\n",
        "before_finetuning = {}\n",
        "\n",
        "for prompt in prompts:\n",
        "\n",
        "    batch = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    generate_ids = model.generate(batch.input_ids, max_length=20)\n",
        "\n",
        "    before_finetuning[prompt] = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    print(\"\\nOutput:\", tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
        "\n",
        "    # batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "    # for i in range(20):\n",
        "    #     next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    #     batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    #     batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "    # before_finetuning[prompt] = tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist())\n",
        "    # print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RKAI93OtBqd"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module  # pre-trained (frozen) linear layer\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
        "        #  <YOUR CODE HERE>\n",
        "        pretrained_output = self.module(input)\n",
        "        lora_output = torch.matmul(input, torch.matmul(self.adapter_A, self.adapter_B))\n",
        "        return pretrained_output + lora_output # <YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbczCQfhQirE"
      },
      "outputs": [],
      "source": [
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "        # extra adapter on MLP components (mlp.*)\n",
        "        module.mlp.up_proj = LoRALayer(module.mlp.up_proj, rank=lora_rank).to(device)\n",
        "        module.mlp.gate_proj = LoRALayer(module.mlp.gate_proj, rank=lora_rank).to(device)\n",
        "        module.mlp.down_proj = LoRALayer(module.mlp.down_proj, rank=lora_rank).to(device)\n",
        "\n",
        "# extra adapter on lm_head\n",
        "# model.lm_head = LoRALayer(model.lm_head, rank=lora_rank).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P12S918rnma5"
      },
      "outputs": [],
      "source": [
        "def crop_lines_512(example):\n",
        "    example['content'] = example['content'][:512]\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y5GUEUlSJ8MJ",
        "outputId": "0263136c-dc5a-45e1-8725-54ec0f4b57f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='501' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 1:33:33, Epoch 0.35/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.525000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.463700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.353800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.397500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.353200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.425300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.409000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.349000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.291100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.489200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.493100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.313500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.520400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.452800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.294700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.297500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.237000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.284200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.303200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.428300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.122700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.231800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.196100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.228500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.096100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.270700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.099300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.248900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.321800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.232400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.281700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.190600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.221200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.282900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.350100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.177300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.330900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.298800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.178600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.147000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.119900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.383500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.175300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.033000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.232700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.262400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.377900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.234200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.261300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.302200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.267300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.139500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.236200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.227800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.176300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.360900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.333400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.212800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.122500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.414300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.314800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.354500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.356800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.358400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.442600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.200300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.270100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.216200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.254900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.423200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.357200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.291800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.377300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.111100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.159900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.169700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.128700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.379100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.207600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.959000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.288600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.179600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.934200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.229100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.154300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.169100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.343200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.202800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.281100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.277900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.150100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.379900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.166600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.499700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.323100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.418200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.345700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>1.155600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.264900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.266200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.142300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.250100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.146500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.349600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.094300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.132900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.288900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.150400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.317500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.265200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>1.054200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>1.039600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.165800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>1.241400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.193400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.264900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>1.169200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.278800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>1.191200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.206000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>1.116500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>1.057500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>1.410100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>1.248000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.053800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>1.229200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.075600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>1.176300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.169900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.084400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.277400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>1.409200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>1.240200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>1.367400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.312000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>1.076900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>1.299800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>1.139600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>1.180900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>1.171100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>1.253300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>1.268200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>1.220600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>1.199700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.260300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>1.162400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>1.166900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>1.251900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>1.389300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>1.177700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>1.177400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>1.210200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>1.183600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>1.133700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.249500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>1.177400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>1.130700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>1.342100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>1.197800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>1.147300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>1.171900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>1.232900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>1.330600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>1.395400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.246200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>1.238600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>1.100200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>1.292500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>1.176900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>1.179300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>1.217700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>1.368200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>1.356400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>1.295500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.176200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>1.252200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>1.175200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>1.248000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>1.038300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>1.233900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>1.207800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>1.192300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>1.351500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>1.289800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.381800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>1.225500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>1.257300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>1.197400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>1.305500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>1.128300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>1.347800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>1.123300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>1.162900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.949900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.241300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>1.297200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>1.261400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>1.193700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>1.052100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>1.248600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>1.123000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>1.184600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>1.283600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>1.210400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.181300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>1.281900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>1.120800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>1.070600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>1.214700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>1.211800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>1.060900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>1.281000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>1.215700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>1.292800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.196300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>1.130000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>1.219400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>1.101300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>1.327400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>1.250900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>1.155200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>1.222400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>1.143100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>1.298300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.276900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>1.161500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>1.090400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>1.118000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>1.175600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>1.269700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>1.283000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>1.131000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>1.034300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>1.130700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.284200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>1.319300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>1.104600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>1.475800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>1.032400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>1.392500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>1.262000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>1.207400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>1.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>1.310000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.996800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>1.029900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>1.077600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>1.175900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>1.169800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>1.162100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>1.232800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>1.228400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>1.460400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>1.188600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.138800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>1.083900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>1.294100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>1.324000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>1.212500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>1.245200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>1.149800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>1.241200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>1.212700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>1.027600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.889100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>1.294200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>1.299600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>1.352200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>1.338700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>1.428300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>1.256000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>1.433700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>1.229800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>1.129700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.195500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>1.257600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>1.229700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>1.255400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>1.163100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>1.275600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>1.149500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>1.314700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>1.379800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>1.221000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.314500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>1.340700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>1.200400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>1.154700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>1.264200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>1.352700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>1.291500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>1.223700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>1.138900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>1.205800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.319700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>1.376400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>1.171200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>1.045900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>1.290100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>1.503700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.896500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>1.332500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>1.202600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>1.240400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.228300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>1.343900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>1.365700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>1.173100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>1.341300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>1.185000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>1.161600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>1.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>1.047800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>1.147700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.329900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>1.168900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>1.361600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>1.266200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>1.155700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>1.053000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.990300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>1.259500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>1.094800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>1.231500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.321000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>1.302400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>1.198700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>1.368400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>1.171100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>1.277500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>1.228700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>1.332700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>1.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>1.243000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.323700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>1.157500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>1.245200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>1.089600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.947900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>1.255100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>1.403200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>1.088300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>1.130400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>1.147700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.207300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>1.269900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>1.241500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>1.271900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>1.203000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>1.245200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>1.222700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>1.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>1.473300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>1.145300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.202500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>1.154400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>1.120500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>1.435600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>1.410100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>1.217700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>1.113500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>1.159000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>1.275700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>1.435600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>1.069300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>1.327100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>1.328700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>1.463200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>1.187200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>1.136300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>1.289600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>1.261800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>1.144200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.330100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>1.326200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>1.199000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>1.131800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>1.175100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>1.140900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>1.228100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>1.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>1.198200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>1.070000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.200800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>1.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>1.207600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>1.225100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>1.066600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>1.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>1.187600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>1.264100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>1.324400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>1.200900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.153500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>1.136100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>1.140400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>1.332400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>1.309000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>1.098100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>1.154500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>1.208600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>1.027400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>1.297900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.232100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>1.358800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>1.187800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>1.241800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>1.332000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>1.077700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>1.301400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>1.092600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>1.141300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>1.297200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.271000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>1.075600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>1.110700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>1.174400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>1.293500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>1.272100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>1.375600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>1.144800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>1.193500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>1.219900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.267100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>1.058600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>1.070800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>1.253400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>1.163300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>1.059900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>1.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>1.160100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>1.247400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>1.061500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.364400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>1.187400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>1.192800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>1.405900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>1.286600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>1.041300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>1.126200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>0.952000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>1.032900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>1.182800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>1.075300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>1.136400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>1.254500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>1.186800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>1.047900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>1.114900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>1.124000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>1.193100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>1.366400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.284600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>1.090500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>1.214300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>1.323400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>1.269100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>1.127100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>1.151800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>1.223700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>1.193700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>1.290600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.273800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>1.080700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>1.347900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>1.282800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>1.095500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>1.333500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>1.164500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>1.214000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>1.343600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>1.207100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.171600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>1.336400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>1.239200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>1.225300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>1.096300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>1.129800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>1.237900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>1.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>1.209500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>1.228500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.148100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>1.182900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>1.218700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>1.253400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>1.424500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>1.123000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>1.145300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>1.021500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>1.111000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>1.294100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/integrations/peft.py:389: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "local variable 'active_adapters' referenced before assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, train_dataset\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     11\u001b[0m     args\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/trainer.py:1984\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1984\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/trainer.py:2339\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2336\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/trainer.py:2396\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2394\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   2395\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 2396\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled:\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;66;03m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[39;00m\n\u001b[1;32m   2399\u001b[0m     \u001b[38;5;66;03m# config `stage3_gather_16bit_weights_on_model_save` is True\u001b[39;00m\n\u001b[1;32m   2400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/trainer.py:2901\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2900\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/trainer.py:2959\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2957\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   2958\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2959\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   2961\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2002\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _hf_peft_config_loaded:\n\u001b[1;32m   1999\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2001\u001b[0m     )\n\u001b[0;32m-> 2002\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_adapter_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_peft_format:\n\u001b[1;32m   2005\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2006\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2007\u001b[0m         )\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/integrations/peft.py:415\u001b[0m, in \u001b[0;36mPeftAdapterMixin.get_adapter_state_dict\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_peft_model_state_dict\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     adapter_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m adapter_state_dict \u001b[38;5;241m=\u001b[39m get_peft_model_state_dict(\u001b[38;5;28mself\u001b[39m, adapter_name\u001b[38;5;241m=\u001b[39madapter_name)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_state_dict\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/integrations/peft.py:393\u001b[0m, in \u001b[0;36mPeftAdapterMixin.active_adapter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactive_adapter\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    389\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `active_adapter` method is deprecated and will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     )\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m/cache/selikhanovych/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/integrations/peft.py:383\u001b[0m, in \u001b[0;36mPeftAdapterMixin.active_adapters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# For previous PEFT versions\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mactive_adapters\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    384\u001b[0m     active_adapters \u001b[38;5;241m=\u001b[39m [active_adapters]\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m active_adapters\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'active_adapters' referenced before assignment"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "# https://huggingface.co/datasets/codeparrot/codeparrot-valid-more-filtering\n",
        "data = datasets.load_dataset(\"codeparrot/codeparrot-valid-more-filtering\", split=\"train\") #streaming=True) # 32 lines\n",
        "# take the first 512 characters of each line\n",
        "data = data.map(crop_lines_512)\n",
        "data = data.map(lambda samples: tokenizer(samples['content']), batched=True)\n",
        "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, train_dataset=data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=4,\n",
        "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
        "        warmup_steps=250,\n",
        "        max_steps=500,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs',\n",
        "        # output_dir=\"/content/output_dir\",\n",
        "        # overwrite_output_dir='True',\n",
        "        report_to=None\n",
        "        # save_strategy=\"steps\",\n",
        "        # save_steps=5\n",
        "        ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Yf-leKhRc_c",
        "outputId": "1a84c3b2-c496-41b2-ff71-061c9760a1f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: # -*- coding: utf-8 -*-\n",
            "#\n",
            "# Copyright (\n",
            "\n",
            "Output: import os\n",
            "import sys\n",
            "import time\n",
            "import logging\n",
            "import logging.config\n",
            "import logging\n",
            "\n",
            "Output: from django.db import models\n",
            "from django.contrib.auth.models import User\n",
            "from\n",
            "\n",
            "Output: while True:\n",
            "    try:\n",
            "        # Get the input from the user\n",
            "        input\n",
            "\n",
            "Output: try:\n",
            "    from django.conf.urls import url\n",
            "except ImportError:\n",
            "   \n",
            "\n",
            "Output: if __name__ == '__main__':\n",
            "    import sys\n",
            "    import os\n",
            "\n",
            "\n",
            "Output: for i in range(1, 10):\n",
            "    print(i)\n",
            "   \n",
            "\n",
            "Output: torch.setdefaulttensortype('torch.FloatTensor')\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# after finetuning\n",
        "\n",
        "after_finetuning = {}\n",
        "\n",
        "\n",
        "for prompt in prompts:\n",
        "\n",
        "    batch = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    generate_ids = model.generate(batch.input_ids, max_length=20)\n",
        "    print(\"\\nOutput:\", tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
        "    after_finetuning[prompt] = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    # batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "    # for i in range(20):\n",
        "    #     next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    #     batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    #     batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "    # after_finetuning[prompt] = tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist())\n",
        "    # print(\"\\nOutput_2:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "SSucUeB4ulB9",
        "outputId": "62865262-9633-4ec2-be5a-cd30439357fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table style=\"border:1px solid black\" >\n",
              "  <tr>\n",
              "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">``</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">▶▶ 2019-2020 School Year\n",
              "The 20</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"># -*- coding: utf-8 -*-\n",
              "#\n",
              "# Copyright (</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import Foundation\n",
              "\n",
              "public extension NSURL {\n",
              "    public var absoluteString: String {\n",
              "</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import os\n",
              "import sys\n",
              "import time\n",
              "import logging\n",
              "import logging.config\n",
              "import logging</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import absolute_import\n",
              "from __future__ import division\n",
              "from</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from django.db import models\n",
              "from django.contrib.auth.models import User\n",
              "from</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while(1)\n",
              "while(1) {\n",
              "    // do something\n",
              "}\n",
              "\\</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while True:\n",
              "    try:\n",
              "        # Get the input from the user\n",
              "        input</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try to find the best solution for your needs.\n",
              "We are a team of professionals with</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try:\n",
              "    from django.conf.urls import url\n",
              "except ImportError:\n",
              "   </pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if ( !window.atmosphere ) {\n",
              "    window.atmosphere = {};</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if __name__ == '__main__':\n",
              "    import sys\n",
              "    import os\n",
              "</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for the 2019-2020 school year.\n",
              "The application process</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for i in range(1, 10):\n",
              "    print(i)\n",
              "   </pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torchbearer 2017-05-18 19</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torch.setdefaulttensortype('torch.FloatTensor')\n",
              "\n",
              "</pre></td>\n",
              "  </tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# This template helps to compare generated code samples in pretty table form\n",
        "# feel free to present your work in other forms\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
        "  <tr>\n",
        "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
        "  </tr>\n",
        "{}\n",
        "</table>\"\"\"\n",
        "\n",
        "row_template = '''  <tr>\n",
        "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "  </tr>'''\n",
        "\n",
        "rows = []\n",
        "\n",
        "for prompt in prompts:\n",
        "    # replace placeholders in the format() arguments\n",
        "    rows.append(row_template.format(prompt, before_finetuning[prompt], after_finetuning[prompt]))\n",
        "\n",
        "display(HTML(table_template.format('\\n'.join(rows))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrKidv5KulB9"
      },
      "source": [
        "If you reach this: congratulations! you've completed everything in this practice session.\n",
        "\n",
        "If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n",
        "You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n",
        "\n",
        "\n",
        "\n",
        "### Read more\n",
        "\n",
        "* How post-training quantization works: https://arxiv.org/abs/2208.07339\n",
        "* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n",
        "* A general library for different adapter types: https://adapterhub.ml/\n",
        "\n",
        "\n",
        "### [extra info] Running other models.\n",
        "\n",
        "This notebook's code can run with other models of similar size, such as [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1). However, they will require minor code tweaks:\n",
        "1. change the model name in `AutoModelForCausalLM.from_pretrained()` __and__ `AutoTokenizer`\n",
        "2. In the prompt tuning code, change `model.model.embed_tokens` to refer to the target model's word embeddings. Simply `print(model)` to navigate to them.\n",
        "3. Change code to add Lora layers - specifically where you what the transformer block components, since those components now have different names."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "nlp_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "012d2d2de5b648e184287a82c4a5b4a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0756ee6cb8dc4ea5a2805f0e0d7d2ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e259febd15243b78ce9c8d81c65946c",
            "placeholder": "​",
            "style": "IPY_MODEL_d8f842f7942e41488a73f7d1ce2b5298",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "085b6c7dcd794fa182aa2b6a18dd0c14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "130102f1843042059d72779bba16c51b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "165d73419e5248d1940c7a60f815625f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d675431174cd439c8a8b6a0610ce364d",
              "IPY_MODEL_e16436c4c8b44f11bd1cf8f559cf95ff",
              "IPY_MODEL_62c679549dde4557a61759922de28de5"
            ],
            "layout": "IPY_MODEL_435dd8a2143f4d04bf4f72acad88d75c"
          }
        },
        "1a7debb4a3d949c585a0606cbeec17f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27ec6cec8e7c467a94f044c5a132ee49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b297f4b29ac4dbc9d5f007457920b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0ae52a6ce8e43a0a52e6267ec3b27bb",
            "placeholder": "​",
            "style": "IPY_MODEL_fa6142454e7f4bebb662d4284affbb39",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "435dd8a2143f4d04bf4f72acad88d75c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "536530f44f5448e99bfaea94f55f3ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b297f4b29ac4dbc9d5f007457920b15",
              "IPY_MODEL_6d63e403db9a4d6dacbdec3ae08e088a",
              "IPY_MODEL_a2b4f4b9da5945f383eaa7fbc23bf170"
            ],
            "layout": "IPY_MODEL_27ec6cec8e7c467a94f044c5a132ee49"
          }
        },
        "5e7c20e77b9649f0ad69ca6ca4760f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_085b6c7dcd794fa182aa2b6a18dd0c14",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_012d2d2de5b648e184287a82c4a5b4a5",
            "value": 33
          }
        },
        "62c679549dde4557a61759922de28de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df3af726188c43abbd2853cfc7fab66f",
            "placeholder": "​",
            "style": "IPY_MODEL_a44ebdcc28244d8890e37a9ca15cf0db",
            "value": " 33/33 [01:49&lt;00:00,  3.62s/it]"
          }
        },
        "6bf3f2983ad444f79798ea2f6b9f4f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d63e403db9a4d6dacbdec3ae08e088a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_130102f1843042059d72779bba16c51b",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa46f51405a94efd8860c4d1023112be",
            "value": 33
          }
        },
        "706f7a7dbdc3482a9614be8aea322573": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79fc63080fc14e3c8cc435c7a6ca31cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e259febd15243b78ce9c8d81c65946c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ab3b4df59c4c989795224a10cdb7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_706f7a7dbdc3482a9614be8aea322573",
            "placeholder": "​",
            "style": "IPY_MODEL_1a7debb4a3d949c585a0606cbeec17f8",
            "value": " 33/33 [02:17&lt;00:00,  3.45s/it]"
          }
        },
        "8e49394e45e940428e7c84978164815f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0756ee6cb8dc4ea5a2805f0e0d7d2ce8",
              "IPY_MODEL_5e7c20e77b9649f0ad69ca6ca4760f09",
              "IPY_MODEL_86ab3b4df59c4c989795224a10cdb7e0"
            ],
            "layout": "IPY_MODEL_aee1bb89f43742239ad0ac3b8c0a55d7"
          }
        },
        "9a4db375ca6e40828a61a59cdbd796be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac5c842761e414091f676d3dc190ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2b4f4b9da5945f383eaa7fbc23bf170": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd06a07f64204ae684f917e2cc04e213",
            "placeholder": "​",
            "style": "IPY_MODEL_6bf3f2983ad444f79798ea2f6b9f4f03",
            "value": " 33/33 [02:34&lt;00:00,  4.01s/it]"
          }
        },
        "a44ebdcc28244d8890e37a9ca15cf0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa46f51405a94efd8860c4d1023112be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aee1bb89f43742239ad0ac3b8c0a55d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd06a07f64204ae684f917e2cc04e213": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0ae52a6ce8e43a0a52e6267ec3b27bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d675431174cd439c8a8b6a0610ce364d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a4db375ca6e40828a61a59cdbd796be",
            "placeholder": "​",
            "style": "IPY_MODEL_79fc63080fc14e3c8cc435c7a6ca31cb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d8f842f7942e41488a73f7d1ce2b5298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df3af726188c43abbd2853cfc7fab66f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e16436c4c8b44f11bd1cf8f559cf95ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ac5c842761e414091f676d3dc190ad3",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa00dc286ee64cef9f84d8debeab18f9",
            "value": 33
          }
        },
        "fa00dc286ee64cef9f84d8debeab18f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa6142454e7f4bebb662d4284affbb39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}